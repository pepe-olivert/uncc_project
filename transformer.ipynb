{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pepe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from whoosh.index import open_dir\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from whoosh.qparser import QueryParser\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForTokenClassification,AutoModelForSequenceClassification\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "#nltk.download(\"punkt\")\n",
    "from keybert import KeyBERT\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import TrainingArguments, AdamW\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sustract_keywords(query):\n",
    "    resul=[]\n",
    "    kw = kw_model.extract_keywords(query,keyphrase_ngram_range=(1, 1), stop_words=None)\n",
    "    for k,v in kw:\n",
    "        resul.append(k)\n",
    "        \n",
    "    return resul\n",
    "\n",
    "def build_query(lista):\n",
    "    new_l=[]\n",
    "    for elem in lista:\n",
    "        \n",
    "        new_l.append(f'evidences:{elem} OR keywords:{elem}')\n",
    "        \n",
    "    return \" OR \".join(new_l)\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "modelSBERT = SentenceTransformer('mitra-mir/setfit-model-Feb11-Misinformation-on-Media-Traditional-Social')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo para sacar las 5 evidencias más cercanas a cada claim del dataset y sacar, obviamente, los embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data=pd.read_csv('iverifyzm.csv')\n",
    "data=data.drop(['Unnamed: 0'], axis=1)\n",
    "for i in range(len(data)):\n",
    "    if data['label'][i]=='True' :\n",
    "        data['label'][i]=2\n",
    "    elif data['label'][i]=='Partly False' or data['label'][i]=='False' or data['label'][i]=='Misleading':\n",
    "        data['label'][i]=0\n",
    "    elif data['label'][i]=='Unproven':\n",
    "        data['label'][i]=1\n",
    "        \n",
    "d=pd.read_csv('africa_check_nigeria.csv')\n",
    "d = d.rename(columns={'claim':'claims'})\n",
    "for i in range(len(d)):\n",
    "    if d['label'][i]=='checked' or d['label'][i]=='Checked':\n",
    "        d['label'][i]=2\n",
    "    elif d['label'][i]=='false' or d['label'][i]=='False' or d['label'][i]=='Fake' or d['label'][i]=='Misleading':\n",
    "        d['label'][i]=0\n",
    "        \n",
    "data=pd.concat([data,d])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cont = 1\n",
    "embedding_train = []\n",
    "evidence_sentences = []\n",
    "for ev in data['evidences']:\n",
    "    print(f\"Analizando {cont}\")\n",
    "    cont += 1\n",
    "    for evs in ev.split('.'):\n",
    "        if len(evs) > 10:\n",
    "            evidence_sentences.append(evs.strip())\n",
    "            embedding_train.append(modelSBERT.encode(evs.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cont = 1\n",
    "top5 = []\n",
    "for claim in data['claims']:\n",
    "    print(f\"Analizando {cont}\")\n",
    "    cont += 1\n",
    "    emb_claim = modelSBERT.encode(claim.strip())\n",
    "    indx = np.flip(np.argsort(cosine_similarity(emb_claim.reshape(1, -1), embedding_train))[0][-5:])\n",
    "    claim5 = []\n",
    "    for i in indx:\n",
    "        claim5.append(evidence_sentences[i])\n",
    "    top5.append(claim5)\n",
    "    \n",
    "newevidences = []\n",
    "for t in top5:\n",
    "    newevidences.append('. '.join(t))\n",
    "newevidences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train = pd.DataFrame(zip(data['claims'], newevidences, data['label']), columns = ['claim', 'top5evidences', 'label'])\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('elections_fact_check_extrafeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: 2887\n",
      "False: 3297\n"
     ]
    }
   ],
   "source": [
    "t=0\n",
    "\n",
    "f=0\n",
    "\n",
    "for i in range(len(train)):\n",
    "    if train['label'][i]==0:\n",
    "        f+=1\n",
    "    elif train['label'][i]==1:\n",
    "        t+=1\n",
    "    \n",
    "        \n",
    "print(f'True: {t}')\n",
    "print(f'False: {f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.to_csv('train.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otros modelos y tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/stsb-roberta-large')\n",
    "\"\"\"model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\", num_labels=3,\n",
    "                                                          ignore_mismatched_sizes=True)\"\"\"\n",
    "#tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vikram71198/distilroberta-base-finetuned-fake-news-detection and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "tokenized_dataset = []\n",
    "for i,row in train[0:1000].iterrows():\n",
    "    tokenized_dataset.append(tokenizer(row[\"claim\"], row['top5_evidences'],\n",
    "                             pad_to_max_length = True,  max_length = 256,return_attention_mask=True,add_special_tokens = True\n",
    "                            ,return_tensors = 'pt'))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vikram71198/distilroberta-base-finetuned-fake-news-detection\", num_labels=3,\n",
    "                                                          ignore_mismatched_sizes=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val,  y_train, y_val = train_test_split(tokenized_dataset,train['label'][0:1000], test_size = 0.2)\n",
    "\n",
    "train_input_ids = torch.cat([d['input_ids'] for d in X_train],dim=0)\n",
    "train_attention_masks = torch.cat([d['attention_mask'] for d in X_train],dim=0)\n",
    "#train_labels = torch.tensor(y_train.map({0: 0,  1: 1,  2: 2}).values)\n",
    "train_labels = torch.tensor(y_train.map({0: 0,  1: 1}).values)\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "#train_dataset = TensorDataset(train_input_ids,  train_labels)\n",
    "\n",
    "val_input_ids = torch.cat([d['input_ids'] for d in X_val],dim=0)\n",
    "val_attention_masks = torch.cat([d['attention_mask'] for d in X_val],dim=0)\n",
    "val_labels = torch.tensor(y_val.map({0: 0,  1: 1}).values)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "#val_dataset = TensorDataset(val_input_ids, val_labels)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Crear un DataLoader que divide el conjunto de datos de val en lotes y les aplica aleatorización y paralelización en la CPU o GPU\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congelamos todas las capas menos las de clasificación, las 5 últimas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"count=0\n",
    "for name, param in model.named_parameters():\n",
    "    count+=1\n",
    "    if count <390:\n",
    "        param.requires_grad = False\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight\n",
      "roberta.embeddings.position_embeddings.weight\n",
      "roberta.embeddings.token_type_embeddings.weight\n",
      "roberta.embeddings.LayerNorm.weight\n",
      "roberta.embeddings.LayerNorm.bias\n",
      "roberta.encoder.layer.0.attention.self.query.weight\n",
      "roberta.encoder.layer.0.attention.self.query.bias\n",
      "roberta.encoder.layer.0.attention.self.key.weight\n",
      "roberta.encoder.layer.0.attention.self.key.bias\n",
      "roberta.encoder.layer.0.attention.self.value.weight\n",
      "roberta.encoder.layer.0.attention.self.value.bias\n",
      "roberta.encoder.layer.0.attention.output.dense.weight\n",
      "roberta.encoder.layer.0.attention.output.dense.bias\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.0.intermediate.dense.weight\n",
      "roberta.encoder.layer.0.intermediate.dense.bias\n",
      "roberta.encoder.layer.0.output.dense.weight\n",
      "roberta.encoder.layer.0.output.dense.bias\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.attention.self.query.weight\n",
      "roberta.encoder.layer.1.attention.self.query.bias\n",
      "roberta.encoder.layer.1.attention.self.key.weight\n",
      "roberta.encoder.layer.1.attention.self.key.bias\n",
      "roberta.encoder.layer.1.attention.self.value.weight\n",
      "roberta.encoder.layer.1.attention.self.value.bias\n",
      "roberta.encoder.layer.1.attention.output.dense.weight\n",
      "roberta.encoder.layer.1.attention.output.dense.bias\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.intermediate.dense.weight\n",
      "roberta.encoder.layer.1.intermediate.dense.bias\n",
      "roberta.encoder.layer.1.output.dense.weight\n",
      "roberta.encoder.layer.1.output.dense.bias\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.attention.self.query.weight\n",
      "roberta.encoder.layer.2.attention.self.query.bias\n",
      "roberta.encoder.layer.2.attention.self.key.weight\n",
      "roberta.encoder.layer.2.attention.self.key.bias\n",
      "roberta.encoder.layer.2.attention.self.value.weight\n",
      "roberta.encoder.layer.2.attention.self.value.bias\n",
      "roberta.encoder.layer.2.attention.output.dense.weight\n",
      "roberta.encoder.layer.2.attention.output.dense.bias\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.intermediate.dense.weight\n",
      "roberta.encoder.layer.2.intermediate.dense.bias\n",
      "roberta.encoder.layer.2.output.dense.weight\n",
      "roberta.encoder.layer.2.output.dense.bias\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.attention.self.query.weight\n",
      "roberta.encoder.layer.3.attention.self.query.bias\n",
      "roberta.encoder.layer.3.attention.self.key.weight\n",
      "roberta.encoder.layer.3.attention.self.key.bias\n",
      "roberta.encoder.layer.3.attention.self.value.weight\n",
      "roberta.encoder.layer.3.attention.self.value.bias\n",
      "roberta.encoder.layer.3.attention.output.dense.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.bias\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.intermediate.dense.weight\n",
      "roberta.encoder.layer.3.intermediate.dense.bias\n",
      "roberta.encoder.layer.3.output.dense.weight\n",
      "roberta.encoder.layer.3.output.dense.bias\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.attention.self.query.weight\n",
      "roberta.encoder.layer.4.attention.self.query.bias\n",
      "roberta.encoder.layer.4.attention.self.key.weight\n",
      "roberta.encoder.layer.4.attention.self.key.bias\n",
      "roberta.encoder.layer.4.attention.self.value.weight\n",
      "roberta.encoder.layer.4.attention.self.value.bias\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.intermediate.dense.weight\n",
      "roberta.encoder.layer.4.intermediate.dense.bias\n",
      "roberta.encoder.layer.4.output.dense.weight\n",
      "roberta.encoder.layer.4.output.dense.bias\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.attention.self.query.weight\n",
      "roberta.encoder.layer.5.attention.self.query.bias\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "roberta.encoder.layer.5.attention.self.value.weight\n",
      "roberta.encoder.layer.5.attention.self.value.bias\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n"
     ]
    }
   ],
   "source": [
    "cont=0\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    if count <101:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Val Acc: 0.88\n",
      "Epoch: 1, Val Acc: 0.89\n",
      "Epoch: 2, Val Acc: 0.89\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [100], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Pasa los datos por el modelo para obtener las predicciones\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#print(output)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Calcula la pérdida\u001b[39;00m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39mlogits, target)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1212\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1210\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1212\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1223\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1224\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    843\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    845\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    846\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    847\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    850\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    851\u001b[0m )\n\u001b[1;32m--> 852\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    864\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    865\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    518\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    520\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:411\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    401\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    408\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 411\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:338\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    330\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    336\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    337\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 338\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    348\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:218\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(hidden_states))\n\u001b[1;32m--> 218\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    220\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[0;32m    222\u001b[0m use_cache \u001b[38;5;241m=\u001b[39m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optim = AdamW(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Número de épocas que deseas entrenar\n",
    "num_epochs = 10\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    # Entrenamiento del modelo\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, masks, target) in enumerate(train_dataloader):\n",
    "        # Reinicia el gradiente acumulado en el optimizador\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # Pasa los datos por el modelo para obtener las predicciones\n",
    "        \n",
    "        output = model(input_ids=inputs, attention_mask=masks, labels=target)\n",
    "        #print(output)\n",
    "        # Calcula la pérdida\n",
    "        loss = loss_fn(output.logits, target)\n",
    "\n",
    "        # Retropropagación\n",
    "        loss.backward()\n",
    "\n",
    "        # Actualiza los parámetros del modelo\n",
    "        optim.step()\n",
    "\n",
    "    # Validación del modelo\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for (inputs, masks, target) in val_dataloader:\n",
    "            # Pasa los datos por el modelo para obtener las predicciones\n",
    "            output = model(input_ids=inputs, attention_mask=masks, labels=target)\n",
    "\n",
    "            # Predice la clase con la mayor probabilidad\n",
    "            _, predicted = torch.max(output.logits, 1)\n",
    "\n",
    "            # Calcula el número de predicciones correctas y el número total de ejemplos\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "        # Calcula la precisión en la validación\n",
    "        val_acc = correct / total\n",
    "\n",
    "    # Imprime la precisión en la validación\n",
    "    print('Epoch: {}, Val Acc: {}'.format(epoch, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del dataset de embedding (claim+top5evidences) + label + sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: 492\n",
      "False: 508\n"
     ]
    }
   ],
   "source": [
    "strain=train[0:1000]\n",
    "t=0\n",
    "\n",
    "f=0\n",
    "\n",
    "for i in range(len(strain)):\n",
    "    if strain['label'][i]==0:\n",
    "        f+=1\n",
    "    elif strain['label'][i]==1:\n",
    "        t+=1\n",
    "    \n",
    "        \n",
    "print(f'True: {t}')\n",
    "print(f'False: {f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrupamos los embeddings utilizando PCA para reducir su dimensión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pepe\\AppData\\Local\\Temp\\ipykernel_15964\\2165557184.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  strain['evclaim']=strain['claim']+strain['top5_evidences']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analizando 1\n",
      "Analizando 2\n",
      "Analizando 3\n",
      "Analizando 4\n",
      "Analizando 5\n",
      "Analizando 6\n",
      "Analizando 7\n",
      "Analizando 8\n",
      "Analizando 9\n",
      "Analizando 10\n",
      "Analizando 11\n",
      "Analizando 12\n",
      "Analizando 13\n",
      "Analizando 14\n",
      "Analizando 15\n",
      "Analizando 16\n",
      "Analizando 17\n",
      "Analizando 18\n",
      "Analizando 19\n",
      "Analizando 20\n",
      "Analizando 21\n",
      "Analizando 22\n",
      "Analizando 23\n",
      "Analizando 24\n",
      "Analizando 25\n",
      "Analizando 26\n",
      "Analizando 27\n",
      "Analizando 28\n",
      "Analizando 29\n",
      "Analizando 30\n",
      "Analizando 31\n",
      "Analizando 32\n",
      "Analizando 33\n",
      "Analizando 34\n",
      "Analizando 35\n",
      "Analizando 36\n",
      "Analizando 37\n",
      "Analizando 38\n",
      "Analizando 39\n",
      "Analizando 40\n",
      "Analizando 41\n",
      "Analizando 42\n",
      "Analizando 43\n",
      "Analizando 44\n",
      "Analizando 45\n",
      "Analizando 46\n",
      "Analizando 47\n",
      "Analizando 48\n",
      "Analizando 49\n",
      "Analizando 50\n",
      "Analizando 51\n",
      "Analizando 52\n",
      "Analizando 53\n",
      "Analizando 54\n",
      "Analizando 55\n",
      "Analizando 56\n",
      "Analizando 57\n",
      "Analizando 58\n",
      "Analizando 59\n",
      "Analizando 60\n",
      "Analizando 61\n",
      "Analizando 62\n",
      "Analizando 63\n",
      "Analizando 64\n",
      "Analizando 65\n",
      "Analizando 66\n",
      "Analizando 67\n",
      "Analizando 68\n",
      "Analizando 69\n",
      "Analizando 70\n",
      "Analizando 71\n",
      "Analizando 72\n",
      "Analizando 73\n",
      "Analizando 74\n",
      "Analizando 75\n",
      "Analizando 76\n",
      "Analizando 77\n",
      "Analizando 78\n",
      "Analizando 79\n",
      "Analizando 80\n",
      "Analizando 81\n",
      "Analizando 82\n",
      "Analizando 83\n",
      "Analizando 84\n",
      "Analizando 85\n",
      "Analizando 86\n",
      "Analizando 87\n",
      "Analizando 88\n",
      "Analizando 89\n",
      "Analizando 90\n",
      "Analizando 91\n",
      "Analizando 92\n",
      "Analizando 93\n",
      "Analizando 94\n",
      "Analizando 95\n",
      "Analizando 96\n",
      "Analizando 97\n",
      "Analizando 98\n",
      "Analizando 99\n",
      "Analizando 100\n",
      "Analizando 101\n",
      "Analizando 102\n",
      "Analizando 103\n",
      "Analizando 104\n",
      "Analizando 105\n",
      "Analizando 106\n",
      "Analizando 107\n",
      "Analizando 108\n",
      "Analizando 109\n",
      "Analizando 110\n",
      "Analizando 111\n",
      "Analizando 112\n",
      "Analizando 113\n",
      "Analizando 114\n",
      "Analizando 115\n",
      "Analizando 116\n",
      "Analizando 117\n",
      "Analizando 118\n",
      "Analizando 119\n",
      "Analizando 120\n",
      "Analizando 121\n",
      "Analizando 122\n",
      "Analizando 123\n",
      "Analizando 124\n",
      "Analizando 125\n",
      "Analizando 126\n",
      "Analizando 127\n",
      "Analizando 128\n",
      "Analizando 129\n",
      "Analizando 130\n",
      "Analizando 131\n",
      "Analizando 132\n",
      "Analizando 133\n",
      "Analizando 134\n",
      "Analizando 135\n",
      "Analizando 136\n",
      "Analizando 137\n",
      "Analizando 138\n",
      "Analizando 139\n",
      "Analizando 140\n",
      "Analizando 141\n",
      "Analizando 142\n",
      "Analizando 143\n",
      "Analizando 144\n",
      "Analizando 145\n",
      "Analizando 146\n",
      "Analizando 147\n",
      "Analizando 148\n",
      "Analizando 149\n",
      "Analizando 150\n",
      "Analizando 151\n",
      "Analizando 152\n",
      "Analizando 153\n",
      "Analizando 154\n",
      "Analizando 155\n",
      "Analizando 156\n",
      "Analizando 157\n",
      "Analizando 158\n",
      "Analizando 159\n",
      "Analizando 160\n",
      "Analizando 161\n",
      "Analizando 162\n",
      "Analizando 163\n",
      "Analizando 164\n",
      "Analizando 165\n",
      "Analizando 166\n",
      "Analizando 167\n",
      "Analizando 168\n",
      "Analizando 169\n",
      "Analizando 170\n",
      "Analizando 171\n",
      "Analizando 172\n",
      "Analizando 173\n",
      "Analizando 174\n",
      "Analizando 175\n",
      "Analizando 176\n",
      "Analizando 177\n",
      "Analizando 178\n",
      "Analizando 179\n",
      "Analizando 180\n",
      "Analizando 181\n",
      "Analizando 182\n",
      "Analizando 183\n",
      "Analizando 184\n",
      "Analizando 185\n",
      "Analizando 186\n",
      "Analizando 187\n",
      "Analizando 188\n",
      "Analizando 189\n",
      "Analizando 190\n",
      "Analizando 191\n",
      "Analizando 192\n",
      "Analizando 193\n",
      "Analizando 194\n",
      "Analizando 195\n",
      "Analizando 196\n",
      "Analizando 197\n",
      "Analizando 198\n",
      "Analizando 199\n",
      "Analizando 200\n",
      "Analizando 201\n",
      "Analizando 202\n",
      "Analizando 203\n",
      "Analizando 204\n",
      "Analizando 205\n",
      "Analizando 206\n",
      "Analizando 207\n",
      "Analizando 208\n",
      "Analizando 209\n",
      "Analizando 210\n",
      "Analizando 211\n",
      "Analizando 212\n",
      "Analizando 213\n",
      "Analizando 214\n",
      "Analizando 215\n",
      "Analizando 216\n",
      "Analizando 217\n",
      "Analizando 218\n",
      "Analizando 219\n",
      "Analizando 220\n",
      "Analizando 221\n",
      "Analizando 222\n",
      "Analizando 223\n",
      "Analizando 224\n",
      "Analizando 225\n",
      "Analizando 226\n",
      "Analizando 227\n",
      "Analizando 228\n",
      "Analizando 229\n",
      "Analizando 230\n",
      "Analizando 231\n",
      "Analizando 232\n",
      "Analizando 233\n",
      "Analizando 234\n",
      "Analizando 235\n",
      "Analizando 236\n",
      "Analizando 237\n",
      "Analizando 238\n",
      "Analizando 239\n",
      "Analizando 240\n",
      "Analizando 241\n",
      "Analizando 242\n",
      "Analizando 243\n",
      "Analizando 244\n",
      "Analizando 245\n",
      "Analizando 246\n",
      "Analizando 247\n",
      "Analizando 248\n",
      "Analizando 249\n",
      "Analizando 250\n",
      "Analizando 251\n",
      "Analizando 252\n",
      "Analizando 253\n",
      "Analizando 254\n",
      "Analizando 255\n",
      "Analizando 256\n",
      "Analizando 257\n",
      "Analizando 258\n",
      "Analizando 259\n",
      "Analizando 260\n",
      "Analizando 261\n",
      "Analizando 262\n",
      "Analizando 263\n",
      "Analizando 264\n",
      "Analizando 265\n",
      "Analizando 266\n",
      "Analizando 267\n",
      "Analizando 268\n",
      "Analizando 269\n",
      "Analizando 270\n",
      "Analizando 271\n",
      "Analizando 272\n",
      "Analizando 273\n",
      "Analizando 274\n",
      "Analizando 275\n",
      "Analizando 276\n",
      "Analizando 277\n",
      "Analizando 278\n",
      "Analizando 279\n",
      "Analizando 280\n",
      "Analizando 281\n",
      "Analizando 282\n",
      "Analizando 283\n",
      "Analizando 284\n",
      "Analizando 285\n",
      "Analizando 286\n",
      "Analizando 287\n",
      "Analizando 288\n",
      "Analizando 289\n",
      "Analizando 290\n",
      "Analizando 291\n",
      "Analizando 292\n",
      "Analizando 293\n",
      "Analizando 294\n",
      "Analizando 295\n",
      "Analizando 296\n",
      "Analizando 297\n",
      "Analizando 298\n",
      "Analizando 299\n",
      "Analizando 300\n",
      "Analizando 301\n",
      "Analizando 302\n",
      "Analizando 303\n",
      "Analizando 304\n",
      "Analizando 305\n",
      "Analizando 306\n",
      "Analizando 307\n",
      "Analizando 308\n",
      "Analizando 309\n",
      "Analizando 310\n",
      "Analizando 311\n",
      "Analizando 312\n",
      "Analizando 313\n",
      "Analizando 314\n",
      "Analizando 315\n",
      "Analizando 316\n",
      "Analizando 317\n",
      "Analizando 318\n",
      "Analizando 319\n",
      "Analizando 320\n",
      "Analizando 321\n",
      "Analizando 322\n",
      "Analizando 323\n",
      "Analizando 324\n",
      "Analizando 325\n",
      "Analizando 326\n",
      "Analizando 327\n",
      "Analizando 328\n",
      "Analizando 329\n",
      "Analizando 330\n",
      "Analizando 331\n",
      "Analizando 332\n",
      "Analizando 333\n",
      "Analizando 334\n",
      "Analizando 335\n",
      "Analizando 336\n",
      "Analizando 337\n",
      "Analizando 338\n",
      "Analizando 339\n",
      "Analizando 340\n",
      "Analizando 341\n",
      "Analizando 342\n",
      "Analizando 343\n",
      "Analizando 344\n",
      "Analizando 345\n",
      "Analizando 346\n",
      "Analizando 347\n",
      "Analizando 348\n",
      "Analizando 349\n",
      "Analizando 350\n",
      "Analizando 351\n",
      "Analizando 352\n",
      "Analizando 353\n",
      "Analizando 354\n",
      "Analizando 355\n",
      "Analizando 356\n",
      "Analizando 357\n",
      "Analizando 358\n",
      "Analizando 359\n",
      "Analizando 360\n",
      "Analizando 361\n",
      "Analizando 362\n",
      "Analizando 363\n",
      "Analizando 364\n",
      "Analizando 365\n",
      "Analizando 366\n",
      "Analizando 367\n",
      "Analizando 368\n",
      "Analizando 369\n",
      "Analizando 370\n",
      "Analizando 371\n",
      "Analizando 372\n",
      "Analizando 373\n",
      "Analizando 374\n",
      "Analizando 375\n",
      "Analizando 376\n",
      "Analizando 377\n",
      "Analizando 378\n",
      "Analizando 379\n",
      "Analizando 380\n",
      "Analizando 381\n",
      "Analizando 382\n",
      "Analizando 383\n",
      "Analizando 384\n",
      "Analizando 385\n",
      "Analizando 386\n",
      "Analizando 387\n",
      "Analizando 388\n",
      "Analizando 389\n",
      "Analizando 390\n",
      "Analizando 391\n",
      "Analizando 392\n",
      "Analizando 393\n",
      "Analizando 394\n",
      "Analizando 395\n",
      "Analizando 396\n",
      "Analizando 397\n",
      "Analizando 398\n",
      "Analizando 399\n",
      "Analizando 400\n",
      "Analizando 401\n",
      "Analizando 402\n",
      "Analizando 403\n",
      "Analizando 404\n",
      "Analizando 405\n",
      "Analizando 406\n",
      "Analizando 407\n",
      "Analizando 408\n",
      "Analizando 409\n",
      "Analizando 410\n",
      "Analizando 411\n",
      "Analizando 412\n",
      "Analizando 413\n",
      "Analizando 414\n",
      "Analizando 415\n",
      "Analizando 416\n",
      "Analizando 417\n",
      "Analizando 418\n",
      "Analizando 419\n",
      "Analizando 420\n",
      "Analizando 421\n",
      "Analizando 422\n",
      "Analizando 423\n",
      "Analizando 424\n",
      "Analizando 425\n",
      "Analizando 426\n",
      "Analizando 427\n",
      "Analizando 428\n",
      "Analizando 429\n",
      "Analizando 430\n",
      "Analizando 431\n",
      "Analizando 432\n",
      "Analizando 433\n",
      "Analizando 434\n",
      "Analizando 435\n",
      "Analizando 436\n",
      "Analizando 437\n",
      "Analizando 438\n",
      "Analizando 439\n",
      "Analizando 440\n",
      "Analizando 441\n",
      "Analizando 442\n",
      "Analizando 443\n",
      "Analizando 444\n",
      "Analizando 445\n",
      "Analizando 446\n",
      "Analizando 447\n",
      "Analizando 448\n",
      "Analizando 449\n",
      "Analizando 450\n",
      "Analizando 451\n",
      "Analizando 452\n",
      "Analizando 453\n",
      "Analizando 454\n",
      "Analizando 455\n",
      "Analizando 456\n",
      "Analizando 457\n",
      "Analizando 458\n",
      "Analizando 459\n",
      "Analizando 460\n",
      "Analizando 461\n",
      "Analizando 462\n",
      "Analizando 463\n",
      "Analizando 464\n",
      "Analizando 465\n",
      "Analizando 466\n",
      "Analizando 467\n",
      "Analizando 468\n",
      "Analizando 469\n",
      "Analizando 470\n",
      "Analizando 471\n",
      "Analizando 472\n",
      "Analizando 473\n",
      "Analizando 474\n",
      "Analizando 475\n",
      "Analizando 476\n",
      "Analizando 477\n",
      "Analizando 478\n",
      "Analizando 479\n",
      "Analizando 480\n",
      "Analizando 481\n",
      "Analizando 482\n",
      "Analizando 483\n",
      "Analizando 484\n",
      "Analizando 485\n",
      "Analizando 486\n",
      "Analizando 487\n",
      "Analizando 488\n",
      "Analizando 489\n",
      "Analizando 490\n",
      "Analizando 491\n",
      "Analizando 492\n",
      "Analizando 493\n",
      "Analizando 494\n",
      "Analizando 495\n",
      "Analizando 496\n",
      "Analizando 497\n",
      "Analizando 498\n",
      "Analizando 499\n",
      "Analizando 500\n",
      "Analizando 501\n",
      "Analizando 502\n",
      "Analizando 503\n",
      "Analizando 504\n",
      "Analizando 505\n",
      "Analizando 506\n",
      "Analizando 507\n",
      "Analizando 508\n",
      "Analizando 509\n",
      "Analizando 510\n",
      "Analizando 511\n",
      "Analizando 512\n",
      "Analizando 513\n",
      "Analizando 514\n",
      "Analizando 515\n",
      "Analizando 516\n",
      "Analizando 517\n",
      "Analizando 518\n",
      "Analizando 519\n",
      "Analizando 520\n",
      "Analizando 521\n",
      "Analizando 522\n",
      "Analizando 523\n",
      "Analizando 524\n",
      "Analizando 525\n",
      "Analizando 526\n",
      "Analizando 527\n",
      "Analizando 528\n",
      "Analizando 529\n",
      "Analizando 530\n",
      "Analizando 531\n",
      "Analizando 532\n",
      "Analizando 533\n",
      "Analizando 534\n",
      "Analizando 535\n",
      "Analizando 536\n",
      "Analizando 537\n",
      "Analizando 538\n",
      "Analizando 539\n",
      "Analizando 540\n",
      "Analizando 541\n",
      "Analizando 542\n",
      "Analizando 543\n",
      "Analizando 544\n",
      "Analizando 545\n",
      "Analizando 546\n",
      "Analizando 547\n",
      "Analizando 548\n",
      "Analizando 549\n",
      "Analizando 550\n",
      "Analizando 551\n",
      "Analizando 552\n",
      "Analizando 553\n",
      "Analizando 554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analizando 555\n",
      "Analizando 556\n",
      "Analizando 557\n",
      "Analizando 558\n",
      "Analizando 559\n",
      "Analizando 560\n",
      "Analizando 561\n",
      "Analizando 562\n",
      "Analizando 563\n",
      "Analizando 564\n",
      "Analizando 565\n",
      "Analizando 566\n",
      "Analizando 567\n",
      "Analizando 568\n",
      "Analizando 569\n",
      "Analizando 570\n",
      "Analizando 571\n",
      "Analizando 572\n",
      "Analizando 573\n",
      "Analizando 574\n",
      "Analizando 575\n",
      "Analizando 576\n",
      "Analizando 577\n",
      "Analizando 578\n",
      "Analizando 579\n",
      "Analizando 580\n",
      "Analizando 581\n",
      "Analizando 582\n",
      "Analizando 583\n",
      "Analizando 584\n",
      "Analizando 585\n",
      "Analizando 586\n",
      "Analizando 587\n",
      "Analizando 588\n",
      "Analizando 589\n",
      "Analizando 590\n",
      "Analizando 591\n",
      "Analizando 592\n",
      "Analizando 593\n",
      "Analizando 594\n",
      "Analizando 595\n",
      "Analizando 596\n",
      "Analizando 597\n",
      "Analizando 598\n",
      "Analizando 599\n",
      "Analizando 600\n",
      "Analizando 601\n",
      "Analizando 602\n",
      "Analizando 603\n",
      "Analizando 604\n",
      "Analizando 605\n",
      "Analizando 606\n",
      "Analizando 607\n",
      "Analizando 608\n",
      "Analizando 609\n",
      "Analizando 610\n",
      "Analizando 611\n",
      "Analizando 612\n",
      "Analizando 613\n",
      "Analizando 614\n",
      "Analizando 615\n",
      "Analizando 616\n",
      "Analizando 617\n",
      "Analizando 618\n",
      "Analizando 619\n",
      "Analizando 620\n",
      "Analizando 621\n",
      "Analizando 622\n",
      "Analizando 623\n",
      "Analizando 624\n",
      "Analizando 625\n",
      "Analizando 626\n",
      "Analizando 627\n",
      "Analizando 628\n",
      "Analizando 629\n",
      "Analizando 630\n",
      "Analizando 631\n",
      "Analizando 632\n",
      "Analizando 633\n",
      "Analizando 634\n",
      "Analizando 635\n",
      "Analizando 636\n",
      "Analizando 637\n",
      "Analizando 638\n",
      "Analizando 639\n",
      "Analizando 640\n",
      "Analizando 641\n",
      "Analizando 642\n",
      "Analizando 643\n",
      "Analizando 644\n",
      "Analizando 645\n",
      "Analizando 646\n",
      "Analizando 647\n",
      "Analizando 648\n",
      "Analizando 649\n",
      "Analizando 650\n",
      "Analizando 651\n",
      "Analizando 652\n",
      "Analizando 653\n",
      "Analizando 654\n",
      "Analizando 655\n",
      "Analizando 656\n",
      "Analizando 657\n",
      "Analizando 658\n",
      "Analizando 659\n",
      "Analizando 660\n",
      "Analizando 661\n",
      "Analizando 662\n",
      "Analizando 663\n",
      "Analizando 664\n",
      "Analizando 665\n",
      "Analizando 666\n",
      "Analizando 667\n",
      "Analizando 668\n",
      "Analizando 669\n",
      "Analizando 670\n",
      "Analizando 671\n",
      "Analizando 672\n",
      "Analizando 673\n",
      "Analizando 674\n",
      "Analizando 675\n",
      "Analizando 676\n",
      "Analizando 677\n",
      "Analizando 678\n",
      "Analizando 679\n",
      "Analizando 680\n",
      "Analizando 681\n",
      "Analizando 682\n",
      "Analizando 683\n",
      "Analizando 684\n",
      "Analizando 685\n",
      "Analizando 686\n",
      "Analizando 687\n",
      "Analizando 688\n",
      "Analizando 689\n",
      "Analizando 690\n",
      "Analizando 691\n",
      "Analizando 692\n",
      "Analizando 693\n",
      "Analizando 694\n",
      "Analizando 695\n",
      "Analizando 696\n",
      "Analizando 697\n",
      "Analizando 698\n",
      "Analizando 699\n",
      "Analizando 700\n",
      "Analizando 701\n",
      "Analizando 702\n",
      "Analizando 703\n",
      "Analizando 704\n",
      "Analizando 705\n",
      "Analizando 706\n",
      "Analizando 707\n",
      "Analizando 708\n",
      "Analizando 709\n",
      "Analizando 710\n",
      "Analizando 711\n",
      "Analizando 712\n",
      "Analizando 713\n",
      "Analizando 714\n",
      "Analizando 715\n",
      "Analizando 716\n",
      "Analizando 717\n",
      "Analizando 718\n",
      "Analizando 719\n",
      "Analizando 720\n",
      "Analizando 721\n",
      "Analizando 722\n",
      "Analizando 723\n",
      "Analizando 724\n",
      "Analizando 725\n",
      "Analizando 726\n",
      "Analizando 727\n",
      "Analizando 728\n",
      "Analizando 729\n",
      "Analizando 730\n",
      "Analizando 731\n",
      "Analizando 732\n",
      "Analizando 733\n",
      "Analizando 734\n",
      "Analizando 735\n",
      "Analizando 736\n",
      "Analizando 737\n",
      "Analizando 738\n",
      "Analizando 739\n",
      "Analizando 740\n",
      "Analizando 741\n",
      "Analizando 742\n",
      "Analizando 743\n",
      "Analizando 744\n",
      "Analizando 745\n",
      "Analizando 746\n",
      "Analizando 747\n",
      "Analizando 748\n",
      "Analizando 749\n",
      "Analizando 750\n",
      "Analizando 751\n",
      "Analizando 752\n",
      "Analizando 753\n",
      "Analizando 754\n",
      "Analizando 755\n",
      "Analizando 756\n",
      "Analizando 757\n",
      "Analizando 758\n",
      "Analizando 759\n",
      "Analizando 760\n",
      "Analizando 761\n",
      "Analizando 762\n",
      "Analizando 763\n",
      "Analizando 764\n",
      "Analizando 765\n",
      "Analizando 766\n",
      "Analizando 767\n",
      "Analizando 768\n",
      "Analizando 769\n",
      "Analizando 770\n",
      "Analizando 771\n",
      "Analizando 772\n",
      "Analizando 773\n",
      "Analizando 774\n",
      "Analizando 775\n",
      "Analizando 776\n",
      "Analizando 777\n",
      "Analizando 778\n",
      "Analizando 779\n",
      "Analizando 780\n",
      "Analizando 781\n",
      "Analizando 782\n",
      "Analizando 783\n",
      "Analizando 784\n",
      "Analizando 785\n",
      "Analizando 786\n",
      "Analizando 787\n",
      "Analizando 788\n",
      "Analizando 789\n",
      "Analizando 790\n",
      "Analizando 791\n",
      "Analizando 792\n",
      "Analizando 793\n",
      "Analizando 794\n",
      "Analizando 795\n",
      "Analizando 796\n",
      "Analizando 797\n",
      "Analizando 798\n",
      "Analizando 799\n",
      "Analizando 800\n",
      "Analizando 801\n",
      "Analizando 802\n",
      "Analizando 803\n",
      "Analizando 804\n",
      "Analizando 805\n",
      "Analizando 806\n",
      "Analizando 807\n",
      "Analizando 808\n",
      "Analizando 809\n",
      "Analizando 810\n",
      "Analizando 811\n",
      "Analizando 812\n",
      "Analizando 813\n",
      "Analizando 814\n",
      "Analizando 815\n",
      "Analizando 816\n",
      "Analizando 817\n",
      "Analizando 818\n",
      "Analizando 819\n",
      "Analizando 820\n",
      "Analizando 821\n",
      "Analizando 822\n",
      "Analizando 823\n",
      "Analizando 824\n",
      "Analizando 825\n",
      "Analizando 826\n",
      "Analizando 827\n",
      "Analizando 828\n",
      "Analizando 829\n",
      "Analizando 830\n",
      "Analizando 831\n",
      "Analizando 832\n",
      "Analizando 833\n",
      "Analizando 834\n",
      "Analizando 835\n",
      "Analizando 836\n",
      "Analizando 837\n",
      "Analizando 838\n",
      "Analizando 839\n",
      "Analizando 840\n",
      "Analizando 841\n",
      "Analizando 842\n",
      "Analizando 843\n",
      "Analizando 844\n",
      "Analizando 845\n",
      "Analizando 846\n",
      "Analizando 847\n",
      "Analizando 848\n",
      "Analizando 849\n",
      "Analizando 850\n",
      "Analizando 851\n",
      "Analizando 852\n",
      "Analizando 853\n",
      "Analizando 854\n",
      "Analizando 855\n",
      "Analizando 856\n",
      "Analizando 857\n",
      "Analizando 858\n",
      "Analizando 859\n",
      "Analizando 860\n",
      "Analizando 861\n",
      "Analizando 862\n",
      "Analizando 863\n",
      "Analizando 864\n",
      "Analizando 865\n",
      "Analizando 866\n",
      "Analizando 867\n",
      "Analizando 868\n",
      "Analizando 869\n",
      "Analizando 870\n",
      "Analizando 871\n",
      "Analizando 872\n",
      "Analizando 873\n",
      "Analizando 874\n",
      "Analizando 875\n",
      "Analizando 876\n",
      "Analizando 877\n",
      "Analizando 878\n",
      "Analizando 879\n",
      "Analizando 880\n",
      "Analizando 881\n",
      "Analizando 882\n",
      "Analizando 883\n",
      "Analizando 884\n",
      "Analizando 885\n",
      "Analizando 886\n",
      "Analizando 887\n",
      "Analizando 888\n",
      "Analizando 889\n",
      "Analizando 890\n",
      "Analizando 891\n",
      "Analizando 892\n",
      "Analizando 893\n",
      "Analizando 894\n",
      "Analizando 895\n",
      "Analizando 896\n",
      "Analizando 897\n",
      "Analizando 898\n",
      "Analizando 899\n",
      "Analizando 900\n",
      "Analizando 901\n",
      "Analizando 902\n",
      "Analizando 903\n",
      "Analizando 904\n",
      "Analizando 905\n",
      "Analizando 906\n",
      "Analizando 907\n",
      "Analizando 908\n",
      "Analizando 909\n",
      "Analizando 910\n",
      "Analizando 911\n",
      "Analizando 912\n",
      "Analizando 913\n",
      "Analizando 914\n",
      "Analizando 915\n",
      "Analizando 916\n",
      "Analizando 917\n",
      "Analizando 918\n",
      "Analizando 919\n",
      "Analizando 920\n",
      "Analizando 921\n",
      "Analizando 922\n",
      "Analizando 923\n",
      "Analizando 924\n",
      "Analizando 925\n",
      "Analizando 926\n",
      "Analizando 927\n",
      "Analizando 928\n",
      "Analizando 929\n",
      "Analizando 930\n",
      "Analizando 931\n",
      "Analizando 932\n",
      "Analizando 933\n",
      "Analizando 934\n",
      "Analizando 935\n",
      "Analizando 936\n",
      "Analizando 937\n",
      "Analizando 938\n",
      "Analizando 939\n",
      "Analizando 940\n",
      "Analizando 941\n",
      "Analizando 942\n",
      "Analizando 943\n",
      "Analizando 944\n",
      "Analizando 945\n",
      "Analizando 946\n",
      "Analizando 947\n",
      "Analizando 948\n",
      "Analizando 949\n",
      "Analizando 950\n",
      "Analizando 951\n",
      "Analizando 952\n",
      "Analizando 953\n",
      "Analizando 954\n",
      "Analizando 955\n",
      "Analizando 956\n",
      "Analizando 957\n",
      "Analizando 958\n",
      "Analizando 959\n",
      "Analizando 960\n",
      "Analizando 961\n",
      "Analizando 962\n",
      "Analizando 963\n",
      "Analizando 964\n",
      "Analizando 965\n",
      "Analizando 966\n",
      "Analizando 967\n",
      "Analizando 968\n",
      "Analizando 969\n",
      "Analizando 970\n",
      "Analizando 971\n",
      "Analizando 972\n",
      "Analizando 973\n",
      "Analizando 974\n",
      "Analizando 975\n",
      "Analizando 976\n",
      "Analizando 977\n",
      "Analizando 978\n",
      "Analizando 979\n",
      "Analizando 980\n",
      "Analizando 981\n",
      "Analizando 982\n",
      "Analizando 983\n",
      "Analizando 984\n",
      "Analizando 985\n",
      "Analizando 986\n",
      "Analizando 987\n",
      "Analizando 988\n",
      "Analizando 989\n",
      "Analizando 990\n",
      "Analizando 991\n",
      "Analizando 992\n",
      "Analizando 993\n",
      "Analizando 994\n",
      "Analizando 995\n",
      "Analizando 996\n",
      "Analizando 997\n",
      "Analizando 998\n",
      "Analizando 999\n",
      "Analizando 1000\n",
      "Sum of variance ratios:  0.9684270737571378\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'emb_comps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [34], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m pca_model\u001b[38;5;241m.\u001b[39mfit(emb)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum of variance ratios: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28msum\u001b[39m(pca_model\u001b[38;5;241m.\u001b[39mexplained_variance_ratio_))\n\u001b[1;32m---> 15\u001b[0m emb_comps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[43memb_comps\u001b[49m)\n\u001b[0;32m     16\u001b[0m train_ev_label\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m     17\u001b[0m train_ev_label[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mev\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39memb_comps\n",
      "\u001b[1;31mNameError\u001b[0m: name 'emb_comps' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "strain['evclaim']=strain['claim']+strain['top5_evidences']\n",
    "ev=strain['evclaim']\n",
    "emb=[]\n",
    "cont=0\n",
    "for e in ev:\n",
    "    cont+=1\n",
    "    print(f'Analizando {cont}')\n",
    "    \n",
    "    emb.append(modelSBERT.encode(e.strip()))\n",
    "    \n",
    "pca_model=PCA(n_components=50)\n",
    "pca_model.fit(emb)\n",
    "print(\"Sum of variance ratios: \",sum(pca_model.explained_variance_ratio_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_comps = pca_model.transform(emb)\n",
    "emb_comps.shape\n",
    "\n",
    "emb_comps=list(emb_comps)\n",
    "train_ev_label=pd.DataFrame()\n",
    "train_ev_label['ev']=emb_comps\n",
    "train_ev_label['label']=train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=train[list(train.columns)[4:]][0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ev_label=pd.concat([train_ev_label,sent],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ev</th>\n",
       "      <th>label</th>\n",
       "      <th>probFalseZS</th>\n",
       "      <th>probTrueZS</th>\n",
       "      <th>emotion_max</th>\n",
       "      <th>category_max</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>...</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>threat</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>white</th>\n",
       "      <th>Campaign strategies</th>\n",
       "      <th>Election administration</th>\n",
       "      <th>Voter registration</th>\n",
       "      <th>Voter turnout</th>\n",
       "      <th>Women in politics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.10537340865815431, -0.23315448790546983, 0....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>0.027728</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Election administration</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>...</td>\n",
       "      <td>8.902355e-07</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.207005</td>\n",
       "      <td>0.574061</td>\n",
       "      <td>0.053560</td>\n",
       "      <td>0.106275</td>\n",
       "      <td>0.059098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.22703017801758402, -0.13707236571822345, 0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.315393</td>\n",
       "      <td>0.684607</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Campaign strategies</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>...</td>\n",
       "      <td>1.159646e-06</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.504780</td>\n",
       "      <td>0.120901</td>\n",
       "      <td>0.056547</td>\n",
       "      <td>0.190566</td>\n",
       "      <td>0.127206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.43777697114019143, -0.19565933482684034, 0....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.732823</td>\n",
       "      <td>0.267177</td>\n",
       "      <td>disapproval</td>\n",
       "      <td>Campaign strategies</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.009162</td>\n",
       "      <td>...</td>\n",
       "      <td>8.240283e-06</td>\n",
       "      <td>0.015960</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.011057</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.379447</td>\n",
       "      <td>0.113501</td>\n",
       "      <td>0.084640</td>\n",
       "      <td>0.228957</td>\n",
       "      <td>0.193455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.3795883842895938, -0.13462324753925564, 0....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.987513</td>\n",
       "      <td>0.012487</td>\n",
       "      <td>approval</td>\n",
       "      <td>Campaign strategies</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>...</td>\n",
       "      <td>1.097509e-06</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.427851</td>\n",
       "      <td>0.078924</td>\n",
       "      <td>0.086639</td>\n",
       "      <td>0.176794</td>\n",
       "      <td>0.229791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.745929506255487, 0.17149958777612329, -0.04...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573740</td>\n",
       "      <td>0.426261</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Campaign strategies</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>...</td>\n",
       "      <td>1.063706e-06</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.413757</td>\n",
       "      <td>0.124321</td>\n",
       "      <td>0.050157</td>\n",
       "      <td>0.234383</td>\n",
       "      <td>0.177382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>[0.719550687332116, 0.15117377243908478, -0.10...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786035</td>\n",
       "      <td>0.213964</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Campaign strategies</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>...</td>\n",
       "      <td>1.060246e-06</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.454844</td>\n",
       "      <td>0.129144</td>\n",
       "      <td>0.095846</td>\n",
       "      <td>0.147722</td>\n",
       "      <td>0.172444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>[-0.42748668246252075, -0.12071436330357133, -...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.935927</td>\n",
       "      <td>0.064073</td>\n",
       "      <td>disapproval</td>\n",
       "      <td>Campaign strategies</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>1.079972e-06</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.660216</td>\n",
       "      <td>0.105366</td>\n",
       "      <td>0.080979</td>\n",
       "      <td>0.085341</td>\n",
       "      <td>0.068099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>[0.7364865079224981, 0.05463787486491473, -0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.352699</td>\n",
       "      <td>0.647301</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Campaign strategies</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>...</td>\n",
       "      <td>1.053700e-06</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.657777</td>\n",
       "      <td>0.099215</td>\n",
       "      <td>0.068443</td>\n",
       "      <td>0.065280</td>\n",
       "      <td>0.109284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>[0.7733090724259504, 0.1652767634955828, -0.11...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.962491</td>\n",
       "      <td>0.037509</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Campaign strategies</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>...</td>\n",
       "      <td>9.267620e-07</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.457006</td>\n",
       "      <td>0.185236</td>\n",
       "      <td>0.044701</td>\n",
       "      <td>0.101398</td>\n",
       "      <td>0.211659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>[0.783600508613081, 0.2469665893367821, -0.143...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.832090</td>\n",
       "      <td>0.167910</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Campaign strategies</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>...</td>\n",
       "      <td>9.769308e-07</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.489692</td>\n",
       "      <td>0.110350</td>\n",
       "      <td>0.057745</td>\n",
       "      <td>0.112837</td>\n",
       "      <td>0.229377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    ev  label  probFalseZS  \\\n",
       "0    [0.10537340865815431, -0.23315448790546983, 0....      0     0.972272   \n",
       "1    [-0.22703017801758402, -0.13707236571822345, 0...      1     0.315393   \n",
       "2    [0.43777697114019143, -0.19565933482684034, 0....      0     0.732823   \n",
       "3    [-0.3795883842895938, -0.13462324753925564, 0....      0     0.987513   \n",
       "4    [0.745929506255487, 0.17149958777612329, -0.04...      0     0.573740   \n",
       "..                                                 ...    ...          ...   \n",
       "995  [0.719550687332116, 0.15117377243908478, -0.10...      1     0.786035   \n",
       "996  [-0.42748668246252075, -0.12071436330357133, -...      1     0.935927   \n",
       "997  [0.7364865079224981, 0.05463787486491473, -0.0...      1     0.352699   \n",
       "998  [0.7733090724259504, 0.1652767634955828, -0.11...      1     0.962491   \n",
       "999  [0.783600508613081, 0.2469665893367821, -0.143...      1     0.832090   \n",
       "\n",
       "     probTrueZS  emotion_max             category_max  admiration  amusement  \\\n",
       "0      0.027728      neutral  Election administration    0.000132   0.000190   \n",
       "1      0.684607      neutral      Campaign strategies    0.000294   0.000132   \n",
       "2      0.267177  disapproval      Campaign strategies    0.000228   0.000323   \n",
       "3      0.012487     approval      Campaign strategies    0.000186   0.000165   \n",
       "4      0.426261      neutral      Campaign strategies    0.000569   0.000149   \n",
       "..          ...          ...                      ...         ...        ...   \n",
       "995    0.213964      neutral      Campaign strategies    0.000589   0.000364   \n",
       "996    0.064073  disapproval      Campaign strategies    0.000047   0.000088   \n",
       "997    0.647301      neutral      Campaign strategies    0.000318   0.000388   \n",
       "998    0.037509      neutral      Campaign strategies    0.000215   0.000085   \n",
       "999    0.167910      neutral      Campaign strategies    0.000151   0.000080   \n",
       "\n",
       "        anger  annoyance  ...  severe_toxicity  sexual_explicit    threat  \\\n",
       "0    0.000024   0.000100  ...     8.902355e-07         0.000014  0.000022   \n",
       "1    0.000056   0.000283  ...     1.159646e-06         0.000018  0.000023   \n",
       "2    0.001817   0.009162  ...     8.240283e-06         0.015960  0.000170   \n",
       "3    0.000047   0.000338  ...     1.097509e-06         0.000017  0.000023   \n",
       "4    0.000014   0.000071  ...     1.063706e-06         0.000017  0.000021   \n",
       "..        ...        ...  ...              ...              ...       ...   \n",
       "995  0.000074   0.000554  ...     1.060246e-06         0.000019  0.000027   \n",
       "996  0.000244   0.001209  ...     1.079972e-06         0.000019  0.000028   \n",
       "997  0.000048   0.000473  ...     1.053700e-06         0.000020  0.000025   \n",
       "998  0.000466   0.000951  ...     9.267620e-07         0.000018  0.000035   \n",
       "999  0.000093   0.001040  ...     9.769308e-07         0.000016  0.000025   \n",
       "\n",
       "     toxicity     white  Campaign strategies  Election administration  \\\n",
       "0    0.000433  0.000061             0.207005                 0.574061   \n",
       "1    0.000350  0.000076             0.504780                 0.120901   \n",
       "2    0.011057  0.000073             0.379447                 0.113501   \n",
       "3    0.000346  0.000085             0.427851                 0.078924   \n",
       "4    0.000380  0.000088             0.413757                 0.124321   \n",
       "..        ...       ...                  ...                      ...   \n",
       "995  0.000478  0.000069             0.454844                 0.129144   \n",
       "996  0.000430  0.000062             0.660216                 0.105366   \n",
       "997  0.000426  0.000068             0.657777                 0.099215   \n",
       "998  0.000929  0.000068             0.457006                 0.185236   \n",
       "999  0.000429  0.000071             0.489692                 0.110350   \n",
       "\n",
       "     Voter registration  Voter turnout  Women in politics  \n",
       "0              0.053560       0.106275           0.059098  \n",
       "1              0.056547       0.190566           0.127206  \n",
       "2              0.084640       0.228957           0.193455  \n",
       "3              0.086639       0.176794           0.229791  \n",
       "4              0.050157       0.234383           0.177382  \n",
       "..                  ...            ...                ...  \n",
       "995            0.095846       0.147722           0.172444  \n",
       "996            0.080979       0.085341           0.068099  \n",
       "997            0.068443       0.065280           0.109284  \n",
       "998            0.044701       0.101398           0.211659  \n",
       "999            0.057745       0.112837           0.229377  \n",
       "\n",
       "[1000 rows x 55 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ev_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nos guardamos el nuevo dataset que se llama train_ev_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ev_label.to_csv('emblabelssentiments.csv', header=True, index=False)\n",
    "#train_ev_label=pd.read_csv('emblabelssentiments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probamos con este nuevo dataset a realizar pruebas con Random Forest o SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(emb_comps,train_ev_label['label'],test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of RFC 0.815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x_train,y_train)\n",
    "print(\"Score of RFC\",rfc.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Classifier has fitted, this process took 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "start = time.time() \n",
    "\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(x_train,y_train)\n",
    "\n",
    "end = time.time()\n",
    "process = round(end-start,2)\n",
    "print(\"Support Vector Machine Classifier has fitted, this process took {} seconds\".format(process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_classifier.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=svm_classifier.predict(x_test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[87, 22],\n",
       "       [10, 81]], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
